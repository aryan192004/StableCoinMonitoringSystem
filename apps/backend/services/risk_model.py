"""
ML Risk Scoring Model - XGBoost Implementation
Trains and deploys depeg probability prediction model
"""

import numpy as np
import pandas as pd
from typing import Dict, Tuple, Any
from datetime import datetime
import os
import joblib
from pathlib import Path

try:
    import xgboost as xgb

    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("XGBoost not installed. Using rule-based scoring fallback.")


class RiskScoringModel:
    """
    XGBoost-based risk scoring model for depeg prediction
    """

    def __init__(self, model_path: str = None):
        self.model = None
        self.model_path = model_path or "models/risk_model_v1.pkl"
        self.feature_names = [
            "peg_deviation",
            "deviation_duration",
            "volatility",
            "liquidity_score",
            "orderbook_imbalance",
            "cross_exchange_spread",
            "volume_anomaly_score",
        ]

        # Try to load existing model
        if os.path.exists(self.model_path):
            self.load_model()

    def train(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: np.ndarray = None,
        y_val: np.ndarray = None,
    ) -> Dict[str, Any]:
        """
        Train XGBoost model on historical/synthetic data

        Args:
            X_train: Features (n_samples, 7)
            y_train: Labels (0=stable, 1=depeg)
            X_val: Validation features
            y_val: Validation labels

        Returns:
            Training metrics and model info
        """
        if not XGBOOST_AVAILABLE:
            raise ImportError("XGBoost not installed. Run: pip install xgboost")

        # XGBoost parameters optimized for hackathon demo
        params = {
            "objective": "binary:logistic",
            "eval_metric": "auc",
            "max_depth": 6,
            "learning_rate": 0.1,
            "n_estimators": 100,
            "subsample": 0.8,
            "colsample_bytree": 0.8,
            "random_state": 42,
            "tree_method": "hist",  # Faster training
        }

        # Train model
        self.model = xgb.XGBClassifier(**params)

        eval_set = [(X_train, y_train)]
        if X_val is not None and y_val is not None:
            eval_set.append((X_val, y_val))

        self.model.fit(X_train, y_train, eval_set=eval_set, verbose=False)

        # Calculate metrics
        train_score = self.model.score(X_train, y_train)
        val_score = self.model.score(X_val, y_val) if X_val is not None else None

        # Feature importance
        feature_importance = dict(zip(self.feature_names, self.model.feature_importances_))

        metrics = {
            "train_accuracy": train_score,
            "val_accuracy": val_score,
            "feature_importance": feature_importance,
            "model_params": params,
            "training_date": datetime.utcnow().isoformat(),
        }

        return metrics

    def predict_risk(self, features: np.ndarray) -> Tuple[int, str, float, float]:
        """
        Predict risk score and depeg probability

        Args:
            features: Feature array (1, 7)

        Returns:
            risk_score: 0-100 integer score
            risk_level: "Low", "Medium", "High", "Critical"
            depeg_probability: Raw probability [0, 1]
            confidence: Model confidence [0, 1]
        """
        if self.model is None:
            # Fallback to rule-based scoring
            return self._rule_based_scoring(features)

        # Get probability prediction
        depeg_probability = self.model.predict_proba(features)[0][1]

        # Convert to 0-100 score
        risk_score = int(depeg_probability * 100)

        # Determine risk level
        if risk_score < 30:
            risk_level = "Low"
        elif risk_score < 60:
            risk_level = "Medium"
        elif risk_score < 80:
            risk_level = "High"
        else:
            risk_level = "Critical"

        # Calculate confidence (simplified)
        # In production, use model uncertainty quantification
        confidence = 0.85 + np.random.uniform(0, 0.1)

        return risk_score, risk_level, depeg_probability, confidence

    def _rule_based_scoring(self, features: np.ndarray) -> Tuple[int, str, float, float]:
        """
        Fallback rule-based scoring when ML model unavailable

        Uses weighted combination of features
        """
        # Extract features
        peg_dev = abs(features[0, 0])
        duration = features[0, 1]
        volatility = features[0, 2]
        liquidity = features[0, 3]
        imbalance = abs(features[0, 4])
        spread = features[0, 5]
        volume_anom = features[0, 6]

        # Weighted scoring (weights based on domain knowledge)
        weights = {
            "peg_deviation": 0.25,
            "duration": 0.15,
            "volatility": 0.15,
            "liquidity": 0.15,
            "imbalance": 0.10,
            "spread": 0.10,
            "volume_anomaly": 0.10,
        }

        # Normalize and score each feature
        scores = {
            "peg_deviation": min(peg_dev / 5.0, 1.0),  # 5% = max
            "duration": min(duration / 180.0, 1.0),  # 3 hours = max
            "volatility": min(volatility / 0.05, 1.0),  # 5% std = max
            "liquidity": 1 - min(liquidity, 1.0),  # Low liquidity = high risk
            "imbalance": imbalance,  # Already normalized
            "spread": min(spread / 0.02, 1.0),  # 2% spread = max
            "volume_anomaly": min(abs(volume_anom) / 5.0, 1.0),  # 5-sigma = max
        }

        # Calculate weighted score
        risk_score = sum(scores[feature] * weight for feature, weight in weights.items())

        risk_score = int(risk_score * 100)

        # Determine level
        if risk_score < 30:
            risk_level = "Low"
        elif risk_score < 60:
            risk_level = "Medium"
        elif risk_score < 80:
            risk_level = "High"
        else:
            risk_level = "Critical"

        depeg_probability = risk_score / 100.0
        confidence = 0.75  # Lower confidence for rule-based

        return risk_score, risk_level, depeg_probability, confidence

    def get_feature_importance(self) -> Dict[str, float]:
        """
        Get feature importance scores from trained model
        """
        if self.model is None:
            return {}

        return dict(zip(self.feature_names, self.model.feature_importances_))

    def save_model(self, path: str = None):
        """Save trained model to disk"""
        save_path = path or self.model_path

        # Create directory if needed
        Path(save_path).parent.mkdir(parents=True, exist_ok=True)

        joblib.dump(self.model, save_path)
        print(f"Model saved to {save_path}")

    def load_model(self, path: str = None):
        """Load trained model from disk"""
        load_path = path or self.model_path

        if os.path.exists(load_path):
            self.model = joblib.load(load_path)
            print(f"Model loaded from {load_path}")
        else:
            print(f"No model found at {load_path}")


def generate_synthetic_training_data(
    n_samples: int = 10000, depeg_ratio: float = 0.15
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Generate synthetic training data for hackathon demo

    Creates realistic feature distributions with labeled depeg events

    Args:
        n_samples: Number of samples to generate
        depeg_ratio: Proportion of positive (depeg) samples

    Returns:
        X: Feature matrix (n_samples, 7)
        y: Labels (n_samples,) - 0=stable, 1=depeg
    """
    n_depeg = int(n_samples * depeg_ratio)
    n_stable = n_samples - n_depeg

    # Generate stable samples (normal market conditions)
    X_stable = np.array(
        [
            # Peg deviation: mostly between -0.5% to +0.5%
            np.random.normal(0, 0.3, n_stable),
            # Duration: mostly short
            np.random.exponential(10, n_stable),
            # Volatility: low
            np.random.gamma(2, 0.002, n_stable),
            # Liquidity: healthy
            np.random.gamma(5, 0.2, n_stable),
            # Imbalance: balanced
            np.random.normal(0, 0.2, n_stable),
            # Spread: tight
            np.random.gamma(2, 0.001, n_stable),
            # Volume anomaly: normal
            np.random.normal(0, 1.5, n_stable),
        ]
    ).T

    # Generate depeg samples (crisis conditions)
    X_depeg = np.array(
        [
            # Peg deviation: large
            np.random.choice([-1, 1], n_depeg) * np.random.gamma(3, 1.5, n_depeg),
            # Duration: longer
            np.random.exponential(60, n_depeg),
            # Volatility: high
            np.random.gamma(3, 0.01, n_depeg),
            # Liquidity: stressed
            np.random.gamma(2, 0.1, n_depeg),
            # Imbalance: strong sell pressure
            -np.random.beta(5, 2, n_depeg),
            # Spread: wide
            np.random.gamma(3, 0.003, n_depeg),
            # Volume anomaly: spikes
            np.random.gamma(3, 1.5, n_depeg),
        ]
    ).T

    # Combine and shuffle
    X = np.vstack([X_stable, X_depeg])
    y = np.array([0] * n_stable + [1] * n_depeg)

    # Shuffle
    indices = np.random.permutation(n_samples)
    X = X[indices]
    y = y[indices]

    return X, y


def train_hackathon_model(save_path: str = "models/risk_model_v1.pkl"):
    """
    Train XGBoost model on synthetic data for hackathon demo

    This creates a pre-trained model that can be used immediately
    without requiring real historical depeg data
    """
    print("Generating synthetic training data...")
    X_train, y_train = generate_synthetic_training_data(n_samples=10000)
    X_val, y_val = generate_synthetic_training_data(n_samples=2000)

    print(f"Training samples: {len(X_train)} ({sum(y_train)} depeg events)")
    print(f"Validation samples: {len(X_val)} ({sum(y_val)} depeg events)")

    print("\nTraining XGBoost model...")
    model = RiskScoringModel(model_path=save_path)
    metrics = model.train(X_train, y_train, X_val, y_val)

    print("\nTraining Results:")
    print(f"  Train Accuracy: {metrics['train_accuracy']:.4f}")
    print(f"  Val Accuracy: {metrics['val_accuracy']:.4f}")

    print("\nFeature Importance:")
    for feature, importance in sorted(
        metrics["feature_importance"].items(), key=lambda x: x[1], reverse=True
    ):
        print(f"  {feature:25s}: {importance:.4f}")

    # Save model
    model.save_model()

    print(f"\nâœ… Model trained and saved to {save_path}")

    return model, metrics


# Example usage and test
if __name__ == "__main__":
    # Train model
    model, metrics = train_hackathon_model()

    # Test prediction with synthetic features
    print("\n" + "=" * 50)
    print("Testing model predictions...")
    print("=" * 50)

    # Test case 1: Normal conditions (should be Low risk)
    features_normal = np.array(
        [
            [
                0.1,  # Small peg deviation
                2.0,  # Short duration
                0.005,  # Low volatility
                0.8,  # Good liquidity
                0.05,  # Balanced order book
                0.001,  # Tight spread
                1.0,  # Normal volume
            ]
        ]
    )

    score, level, prob, conf = model.predict_risk(features_normal)
    print(f"\nTest 1 - Normal Conditions:")
    print(f"  Risk Score: {score}/100")
    print(f"  Risk Level: {level}")
    print(f"  Depeg Probability: {prob:.2%}")
    print(f"  Confidence: {conf:.2%}")

    # Test case 2: Moderate stress (should be Medium/High risk)
    features_stress = np.array(
        [
            [
                2.5,  # Larger deviation
                45.0,  # Longer duration
                0.02,  # Higher volatility
                0.4,  # Reduced liquidity
                -0.4,  # Sell pressure
                0.008,  # Wider spread
                3.5,  # Volume spike
            ]
        ]
    )

    score, level, prob, conf = model.predict_risk(features_stress)
    print(f"\nTest 2 - Moderate Stress:")
    print(f"  Risk Score: {score}/100")
    print(f"  Risk Level: {level}")
    print(f"  Depeg Probability: {prob:.2%}")
    print(f"  Confidence: {conf:.2%}")

    # Test case 3: Crisis (should be Critical risk)
    features_crisis = np.array(
        [
            [
                8.0,  # Large deviation
                120.0,  # Long duration
                0.08,  # Very high volatility
                0.15,  # Low liquidity
                -0.75,  # Strong sell pressure
                0.02,  # Wide spread
                6.0,  # Extreme volume
            ]
        ]
    )

    score, level, prob, conf = model.predict_risk(features_crisis)
    print(f"\nTest 3 - Crisis Scenario:")
    print(f"  Risk Score: {score}/100")
    print(f"  Risk Level: {level}")
    print(f"  Depeg Probability: {prob:.2%}")
    print(f"  Confidence: {conf:.2%}")
